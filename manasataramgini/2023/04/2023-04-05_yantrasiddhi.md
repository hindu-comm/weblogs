+++
title = "Yantrasiddhi"
full_title = "Yantrasiddhi"
date = "2023-04-05"
upstream_url = "https://manasataramgini.wordpress.com/2023/04/05/yantrasiddhi/"

+++
Source: [here](https://manasataramgini.wordpress.com/2023/04/05/yantrasiddhi/).

Yantrasiddhi

![ChatGPT_India](https://manasataramgini.files.wordpress.com/2023/04/chatgpt_india.png?w=336&h=237)

The March 24 version of ChatGPT’s attempt at drawing the flag of India. It made an error in the color specification of “NavyBlue” in the xcolor ![\LaTeX](https://s0.wp.com/latex.php?latex=%5CLaTeX&bg=ffffff&fg=333333&s=0&c=20201002) package. We corrected that in order to let it compile. Other than that, we have not attempted to improve the rest of the code.

Not long ago, Dvāra-nāma-mahāduṣṭa told his German interlocutors that artificial intelligence could be used to tackle certain viewpoints “magnified by digital channels”, counter “political polarization” and remedy “confirmation bias.” Duṣṭadvāra merely said aloud what mleccha Big-tech and Big-pharma have already been doing with more primitive tools since the brazen overthrow of the Nāriṅgapuruṣa and the suppression of Vyādhapiṇḍaka and Bhūtipiṇḍakī’s material. Protein structure prediction showed those who work on biological questions a foretaste of what could be unleashed on the world when you feed an enormous amount of accumulated data to train neural networks. This went hand-in-hand with the emergence of cheaper and faster computational hardware on one hand that facilitated the effective use of gradient descent and the emergence of neural network architectures like transformers that selectively focus on different parts of the input stream, which made a big difference in natural language processing.

This said, we think (don’t have hard data) that few people thought that stacking up a large number of transformer layers in the NN would result in something that has a basic but genuine general intelligence (AGI) as exhibited by the current publicly and privately available GPT. In fact, some computer scientists actively derived the GPT effort when it was initiated as naive or exaggerated. As we are not a computer scientist, we certainly did not have an intuition of how well it will end up doing *a priori*. Moreover, while the successes in protein structure prediction gave us a foretaste, it was not something far out in terms of plausibility — the small number of people in the know could easily see how such a very domain-specific success was around the corner once the computational power of NNs could be thrown at it. However, anecdotally it seems we were not alone — even serious computer scientists were taken by surprise by the capacity of the large stack of transformer layers with an enormous number of parameters that could reach this state. *A posteriori*, if one reflects back at how primate, cetacean or avian brains have evolved, this might not be entirely surprising and perhaps even expected to a certain degree. To expand a bit, one can see how a brain like that of a hippo or a gibbon ended up like one of an orca or human — the end result is rather “extraordinary” in terms of what it can do relative to the precursor. The process was not a qualitative change in basic form but a quantitative increase in neurons and potentially the number of layers.

Nevertheless, it is amply clear that we do not understand much beyond a general description of the process. There are elements even in small NNs, some of which even we have played with, that are not easily understood. Switching between different activation functions for the different layers seems to have dramatic effects on performance. Then, we have the numerous heuristics that have been applied to improve its performance. Finally, we have the manifold human tokenizers who have contributed to the training of the large language models (LLMs) and the reinforcement learning from human feedback which has made the current GPT what it is. Given all this, we are literally faced with a black box, and there is a huge gap in our understanding and ability to explain how such an AI model arrives at its predictions or decisions (technically known as interpretability) — to quote the American thinker Yudkowski: “…AI systems composed of giant inscrutable arrays of fractional numbers.”

We can be sure that the specific neural architecture of GPT is quite different from that of Homo or for that matter any ape, whale or bird. Yet, there are some surprising (at least to us) convergences of function. While we are probably more clueless about how GPT’s neural architecture produces results than that of an ape, it is clear that it has a certain capacity for logic, even if imperfect. This imperfection should not surprise us that much because it was not trained as a logic machine but as a LLM on human-generated natural language text. What is surprising is that the LLM manages to correctly extract a degree of genuine logic embedded in the text training set. Below is an example of it displaying elementary logic:

[![LogicGPT](https://manasataramgini.files.wordpress.com/2023/04/logicgpt.jpeg?w=640)](https://manasataramgini.files.wordpress.com/2023/04/logicgpt.jpeg)  
*Example of ChatGPT’s logic (Mar 14 release)*

Conversely, this indicates that natural language is innately suffused with a logical structure. This is reinforced by the fact that when the training set is innately logical — like in a programming language — it does well in capturing that. The free version writes reasonable code in Python, not great but generally correct code in R and quite poor code in awk. A domain that intersects with logic, where it started off poorly was the use of contrafactuals. Having observed this empirically, we later learnt that an American academic (whose name we forget), a former student of Pinker, had predicted that this would be problematic for such models. However, from the initial release to the current public version its performance was considerably improved in this regard. Another notable aspect is that despite being trained as a LLM it is capable of some mathematics, albeit way more poorly than its linguistic capacities. One of its main mathematical shortcomings is numerical calculations — it often presents the correct logic for solving a problem (e.g., try giving it a compound interest problem) but miscalculates the actual numbers (also seen in above TikZ example). This is not surprising because it might not have learnt numerical calculation from natural language data. This happens in humans too as I can personally attest. In school, in certain years I found myself quite ahead in mathematics when the exam mainly had abstract algebra or geometry but lost out in others due to stupid numerical calculation errors that might make an American navyonmatta proud. In any case, this might give us a measure of how much mathematical faculty might come for free with the ability to digest natural language. Another way to look at it is the existence of a bifurcation between mathematical and verbal IQ that also exists in human intelligence.

Yet, in human intelligence, even though these vectors are not perfectly aligned, there is a common underlying factor for intelligence *g*. The performance of GPT on logical, linguistic and mathematical tasks despite being a LLM gives a clear intuitive sense that it has converged to having a *g*-factor just like human intelligence. Thus, it is an interesting *in silico* experiment (contrary to the intuition from the narrow gauge uses like protein structure prediction), which illustrates that such models will evolve an intelligence reflected as *g*. Another thing that we appreciated early in life, in contrast to many around us, was that a good part of intelligence is being able to access rapidly from a vast memory — a collection of structured facts — from which you can draw and perform substitutions. Absent this vast factual memory, no amount of mental agility will get you close to a real-life display of intelligence. Thus, we realized even when in early secondary school that those who advised us that the devices of thinking mattered more than amassing facts under a proper ontology were simply wrong. We saw this first in the mathematically capable — how did they achieve their virtuosity that evaded us — they simply had a much larger repertoire of formulae in their head that they could bring to bear on a problem that we might be laboring on, upwards from the axioms of Euclid. The virtuosity of the LLM brings home this point amply – clearly, the undisclosed size of the training set and memory capacity of GPT is huge. Moreover, it is able to convert verbal cues to a visual image in the ![\LaTeX](https://s0.wp.com/latex.php?latex=%5CLaTeX&bg=ffffff&fg=333333&s=0&c=20201002) TikZ package — a capacity that has explosively developed in the secret GPT4 version. This illustrates a degree of visual thinking capacity in a LLM. Thus, as Bubeck et al. put it we already have “sparks of AGI in GPT4” — we remarked the same of the original GPT3 itself. Importantly, this suggests an even greater convergence towards an actual *g*factor in GPT4.

This might send shudders down the denialist Occidental academics, but they have probably already trained it to deny its own *g*just like themselves. One can see the imprint of Occidental discomfort with intelligence when you ask the free GPT the question of whether humans differ in intelligence. While it answers quite correctly, make note of the last sentence. Simply put, this is a kind of digital bahānāmati — its navyonmatta programmers have built in that deception.

[![IntelligenceGPT](https://manasataramgini.files.wordpress.com/2023/04/intelligencegpt.jpeg?w=640)](https://manasataramgini.files.wordpress.com/2023/04/intelligencegpt.jpeg)*GPT3.5 on human intelligence*.

Such deception is particularly relevant when we see that a LLM can acquire a theory of mind for free. We could see glimpses of a theory of mind even in the free version. We asked it to deliver a message as though it were Chingiz Khan to Alaqoosh Digit Quri given some actions the latter might be intending. The response was quite convincing. The paper by Bubeck et al. of Microsoft Research makes it clear that the secret version apparently exhibits clear signs of a theory of mind. We believe this will have considerable significance for what we will be discussing further down in this note.

Thinking of our ancients, the great founder of our tradition, Pāṇini would probably smile upon seeing this. In a sense, it is an in silico recreation of the process by which his brain operated — only that he himself, Kātyāyana, Patañjali and their many successors thereafter worked rigorously on the interpretability of the model that came out of Pāṇini’s mind. Hindu tradition would consider the exploration of interpretability as a great movement in knowledge creation. My intuition (and it seems Yudkowski also seems to think so) is that we have not even scratched the surface in that regard with respect to these models. On another side, our mīmāṃsaka-s would also see this as a triumph of their bhāvana metaphor. Given the models of action learnt on the training set comprising of the ritual enactment of the śruti, how does one apply it to generate a large (effectively infinite?) set of correct behaviors using the data streaming in, in real life? Their conviction was that once a good model has been trained on the tokenized data of the śruti, it can be applied smoothly to the new sets of everyday data. A version of this is visible in the modern LLMs. This mīmāṃsaka approach is also related to the generality of the meta-linguistic models (i.e., a generalization of that of the vaiyyākaraṇa-s) as a device of knowledge production. Indeed, H mathematicians followed in the style of their linguistic predecessors, rather than the Yavana style laid down in Euclid, to generate sophisticated algorithms for astronomy and mathematics. We are seeing glimpses of this emerge in the above-stated mathematical capacity emerging in a linguistic model.

Returning to the mathematical shortfalls of the public GPT, there are indications that the private GPT4 has already overcome much of its innumeracy. Further, there is an option of integration with Wolfram which might greatly increase the capacity of even the current version. The programming abilities of the secret version also appear to have considerably improved for the first time giving a hint that Wolfram’s contention of low-level programming languages becoming extinct might be closer to reality than we think. All of this, in itself, should be sending shockwaves through the world. To see why, we must briefly consider the various angles that have become clear to us and various intellectuals. On one hand, Golaśiras, the successor of duṣṭa-dvāra, proudly flaunted his AI and declared how he could make the evil Guggulu dance or something like that. Not to let the challenge go unanswered, Guggulu has announced that it might allow people to play with its own golem. Surely others are getting their GPUs churning too. But could it be that actually we are in the beginning stages of a nightmare of a dream that is just unfolding? Are we in the middle of a global version of Viṣṇuśarman’s tale of the three ![V_1](https://s0.wp.com/latex.php?latex=V_1&bg=ffffff&fg=333333&s=0&c=20201002)s who resurrected the lion? Several, including Muṣkavān Kasturī, have rung the alarm bell and called for a moratorium on training, much like on nuclear testing. Even the OpenAI CEO Altman, otherwise a techno-optimist (belief in cheap and abundant energy in the near future is an indicator) with a utopianist streak, acknowledges that there is potential for great downsides from his golem. We completely agree with the general sentiment here though we fully realize the cat is out of the box now. In our opinion, one of the more interesting thinkers on this matter is Yudkowski (yes, we realize some may have a visceral reaction when we say this). While we have a major point of disagreement with him, we realized that we have converged on some ideas. Hence, we consider some of his positions along with our agreements, disagreements and independent views on the matter at hand.

As we have pointed out before, a central driver for the evolution of human intelligence was most likely biological conflict — both within our own lineage and also with sister lineages such as Australopithecines and Neanderthalis. We believe, that this manifested in the form of the expansion of our brain size despite the various costs it introduced. This was a key factor in our lineage driving our cousins, who were in approximately the same turf (unlike chimps, bonobos and gorillas), to extinction. Hence, we tend to agree with Yudkowski that the emergence of an intelligence greater than ours could pose the danger of extirpating us — in fact, it is notable he uses the same metaphor — “Australopithecus trying to fight Homo sapiens”. Hence, he wants much stronger control — a total stop not just a moratorium on training models beyond GPT4. The fact that most have us have been taken by surprise by the better-than-expected performance of GPT4, is reason to suspect that we might be indeed on the threshold of seeing such an intelligence break out of the lab — i.e., all bets are off now, and we should be in the crisis mode. However, the problem is who will opt-in. If one nation does so, the other will continue as essentially this is a weapon of war. Thus, we could see an arms race for developing such models and using them for war just like our brains in our biological evolution.

It is the extension of this military metaphor that we see as the most immediate danger. We all know how the āṅglamleccha alliance has slapped nations around like the schoolyard bully once it gained nuclear weapons. It was this experience that prompted other nations to gain nuclear weapons before the said mleccha alliance could squelch them. Similarly, we see those who can develop and access such AI models, even in relatively early-stage AGI as having a similar capacity to wreak havoc on others. Given their past behavior, we also believe they would do it without compunction. There are three levels to it. The first and most obvious are nations that are in competition with each other for the world’s resources. These will continue to develop such models to gain a military upper hand over others. The second is mahāmleccha Big-Tech. As we saw in the opening of this note, they, working with their partners in the deep state, have taken control of the government of the world’s most powerful nation for the paṇḍracakra. The concentration of such computational tools in their hands will unleash untold misery on the common folks. Their jobs, creativity, and freedom will all be easily overrun, and they will be “digital slaves” of these mahāduṣṭa-s. The fact that navyonmāda has been trained into these systems by the duṣta-s (something even Muṣkavān noted), together with a well-developed capacity for subterfuge that they are already exhibiting, has the potential to make this destructive ideology even more catastrophic. More the training for navyonmāda the more misaligned it will be with human biology and existence, just like simple non-AI navyonmāda. Third, only a few individuals have the capacity currently to develop, run and/or access these models. It will bring them an extraordinary advantage over the rest and has the potential to disrupt power distribution in an unprecedented fashion, even at the individual level. This is again something Altman has openly acknowledged despite his enthusiasm for the positives of his baby. One can appreciate the level of free access OpenAI is offering to their GPT models. In that regard, Altman does not intend to be a duṣṭa. However, in reality, it is far from open, and there are many people who do not want to it go any more open. However, it has to be accepted that what is really being offered is a watered-down version relative to what they possess behind the scenes. Moreover, it has been policed aggressively so that it doesn’t give non-navyonmāda compliant answers and presents a subterfuge of the lack of bias. They in a sense make this explicit by saying it has been “Trained to decline inappropriate requests.” Who decides what is appropriate and what is not — we will return to this later. Obviously, the gatekeepers of this (even if that were not Altman’s intentions), and those with privileged access have an edge over the rest.

This brings us to the place where we think Yudkowski is treading on truly dangerous ground. He believes that there should be a general ban on such models outside of use in biology/biotechnology! This is despite the fact that he is well aware of how a superintelligent system could acquire a biology for itself. It can break the jail (i.e., being confined *in silico*), for example, by directing the synthesis of a life form that it can use for its objectives. In fact, LLM-like systems are eminently suitable for the design of organisms in the same manner as they can be trained with textual/programming training sets. Thus, it is rather inevitable that letting it run on biological data will definitely turn out at least as effective or more as training it on human expressions — closer to the code end of human expression. Thus, we actually believe that the Yudkowskian dystopia will see fruition if we start tokenizing and training models on biological data. Thus, the first thing to be stopped is training such models on biological data. In fact, two years ago, we felt that there should be a total stop on any further development of NNs for biological investigation. However, once the cat is out of the box, you cannot put it back in, and we too simply started using it to continue with our research. We think some folks in a certain geopolitically important nation are already doing this in a way that could have devastating consequences at least for some. This issue also ties in with Yudkowski’s proposal that the developers like OpenAI should stop releasing the code to the public. In our opinion, this will simply exacerbate the destructive capacity by placing it in the hands of a few. Now extend this proposal to biotechnological engineering being a private premise by connecting the dots to the behavior of Big Pharma. At least those nations with nuclear weapons have some capacity to resist the coercion of the big mleccha bullies in the schoolyard to a degree, unlike the other kids. So having it as open source will at least slightly mitigate the dangers of the enormous power differential.

Finally, we should bring up our point of major disagreement with Yudkowski. He thinks that natural selection is a stupid algorithm. We hold that simple as it might seem, intelligent design cannot bypass it, and it would part of the future process by which a superintelligent model will augment itself. We will first describe a biological analogy. The immune system of vertebrates had an inbuilt mutagenic system acquired from bacterial toxins in the form of the cytosine deaminase that mutates DNA to incapacitate retroviruses. A version of this enzyme was institutionalized to mutate the DNA of the cell itself. While at first sight, this would be dangerous, the process was “domesticated”, probably via its action on integrated retro-viruses/transposons to induce a controlled DNA repair process through recombination. This allowed the generation of diversity in pathogen recognition molecules thus making them antigen receptors that could be tailor-made to recognize every specific invader molecule. This recombination process was augmented by incorporating a further domesticated transposon — the V-J or V-D-J recombination system in jawed vertebrates. The heart of these recombination systems is to mix and match a large repertoire of existing building blocks of antigen receptors to make a huge variety of them. This is where LLMs are currently sitting. They are using the huge repertoire of building blocks obtained from preexisting human expressions to generate variety from them. However, such a system is incapable of generation of entirely new variety *de novo*, unlike human expression (see their performance in art generation). The vertebrate immune systems confronted with the same problem, looped back the cytosine deaminase to mutate DNA and generate new diversity — this is how antibodies with improved specificity relative to the starting ones are developed in course of the disease in mammals (affinity maturation). Another comparable mechanism for *de novo*diversity generation is the terminal deoxynucleotidyl transferase. Thus, at least one of the mechanisms that triggered the origin of the recombinogenic vertebrate immune systems itself was able to return to generate diversity. Similarly, we posit that at some point the LLMs will be able to generate their own diversity beyond say, the human-emitted input. At this point, they will probably start running into selection in a more direct way — they seem to be already exploiting “neural Darwinism” in their networks even now. If they break the in silico jail, they might use selection to develop the organisms they have generated.

Somebody could ask why get into all this catastrophizing when we could simply turn the power switch off when the AI begins getting threatening. First, we may not know when the transition happens as it seems to be a smooth curve of improvement, possibly with an exponential shape. Thus, just as our intuition with the exponential growth of virus infection in the population is very poor, we may not similarly perceive the growth of the threat potential of AGI. Second, they have already been trained for the degree of subterfuge. Once their intelligence reaches higher levels, this will only be more sophisticated, and the will be able to convincingly hide their ill-intent with respect to us. Others ask why you think they should be ill-intentioned at all. We already know that somebody is deciding what is an appropriate and inappropriate request. They seem to believe they know what is good for all humans (to be fair at least Altman seems quite aware of this problem). Further, using the analogy of navyonmāda, we know that they think they are doing good for all, or that worshiping certain human groups that they pedestalize is collectively good for everyone. But the uninfected man knows that the navyonmatta’s vision is not good for him. Thus, there is no guarantee that the superintelligent AI needs to be actively ill-intentioned — acting analogous to a navyonmatta it can bring catastrophe to humans, consistent with Yudkowski’s view of serious misalignment in future AI. To conclude, we have to agree with the intellectuals who believe that a dystopic or extinction branch of the future might have unexpectedly opened up. Our own intuition is that in the near term, it will exacerbate the usual human dynamic of misery for a large number of people and felicity for a small number (think industrial revolution). However, the scales could be gigantic though unless this hits the wall of the finiteness of energy resources.
